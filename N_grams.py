import nltk
import numpy as np
import random
import string

import bs4 as bs
import urllib.request
import re

raw_html = urllib.request.urlopen('https://en.wikipedia.org/wiki/Tennis')
raw_html = raw_html.read()

article_html = bs.BeautifulSoup(raw_html, 'lxml')
article_paragraphs = article_html.find_all('p')
article_text = ''

for para in article_paragraphs:
    article_text += para.text

article_text = article_text.lower()
article_text = re.sub('[^A-Za-z. ]', '', article_text)
print(article_text)

ngrams = {}
## Char 3-gram model
# chars = 4
#
# for i in range(len(article_text)-chars):
#     seq = article_text[i:i+chars]
#     # print(seq)
#     if seq not in ngrams.keys():
#         ngrams[seq] = []
#     ngrams[seq].append(article_text[i+chars])
#
# curr_sequence = article_text[0:chars]
# output = curr_sequence
#
# for i in range(200):
#     if curr_sequence not in ngrams.keys():
#         break
#     possible_chars = ngrams[curr_sequence]
#     next_char = possible_chars[random.randrange(len(possible_chars))]
#     output += next_char
#     curr_sequence = output[len(output) - chars:len(output)]
#
# print(output)

# Word 3-gram model
words = 4
words_tokens = nltk.word_tokenize(article_text)
for i in range(len(words_tokens)-words):
    seq = ' '.join(words_tokens[i:i+words])
    # print(seq)
    if seq not in ngrams.keys():
        ngrams[seq] = []
    ngrams[seq].append(words_tokens[i + words])

curr_sequence = ' '.join(words_tokens[:words])
output = curr_sequence
for i in range(50):
    if curr_sequence not in ngrams.keys():
        break
    possible_words = ngrams[curr_sequence]
    next_word = possible_words[random.randrange(len(possible_words))]
    output += ' ' + next_word
    seq_words = nltk.word_tokenize(output)
    curr_sequence = ' '.join(seq_words[len(seq_words)-words:len(seq_words)])

print(output)